# @package _global_
paths:
  train_bin: "data/a5-alignment/sft_packed/train.bin"
  valid_bin: "data/a5-alignment/sft_packed/valid.bin"
  model_input: "data/a5-alignment/models/Llama-3.1-8B"
  model_output: "sft_checkpoints_ds"
  deepspeed_config: "configs/deepspeed/zero3_cpu_offload.json"

model:
  context_length: 512

training:
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 16
  max_steps: 100
  bf16: true
  gradient_checkpointing: true
  dataloader_num_workers: 4   # (if you added this field; otherwise hardcode in script)
